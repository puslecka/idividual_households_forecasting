{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Low Carbon London (LCL) dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset can be downloaded form the https://data.london.gov.uk/dataset/smartmeter-energy-use-data-in-london-households website. It contains the energy consumption of 5566 households in London from November 2011 to February 2014. The data is collected in half-hourly intervals. Aftern unzipping the data you can read the csv file, however since the data is over 8GB it is easier to read it in chunks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = r\"path_to_your_folder_with_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LCLid</th>\n",
       "      <th>stdorToU</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>KWH/hh (per half hour)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAC000002</td>\n",
       "      <td>Std</td>\n",
       "      <td>2012-10-12 00:30:00.0000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAC000002</td>\n",
       "      <td>Std</td>\n",
       "      <td>2012-10-12 01:00:00.0000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MAC000002</td>\n",
       "      <td>Std</td>\n",
       "      <td>2012-10-12 01:30:00.0000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MAC000002</td>\n",
       "      <td>Std</td>\n",
       "      <td>2012-10-12 02:00:00.0000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MAC000002</td>\n",
       "      <td>Std</td>\n",
       "      <td>2012-10-12 02:30:00.0000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>MAC000002</td>\n",
       "      <td>Std</td>\n",
       "      <td>2012-10-14 00:30:00.0000000</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>MAC000002</td>\n",
       "      <td>Std</td>\n",
       "      <td>2012-10-14 01:00:00.0000000</td>\n",
       "      <td>0.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>MAC000002</td>\n",
       "      <td>Std</td>\n",
       "      <td>2012-10-14 01:30:00.0000000</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>MAC000002</td>\n",
       "      <td>Std</td>\n",
       "      <td>2012-10-14 02:00:00.0000000</td>\n",
       "      <td>0.126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>MAC000002</td>\n",
       "      <td>Std</td>\n",
       "      <td>2012-10-14 02:30:00.0000000</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LCLid stdorToU                     DateTime  KWH/hh (per half hour) \n",
       "0   MAC000002      Std  2012-10-12 00:30:00.0000000                    0.000\n",
       "1   MAC000002      Std  2012-10-12 01:00:00.0000000                    0.000\n",
       "2   MAC000002      Std  2012-10-12 01:30:00.0000000                    0.000\n",
       "3   MAC000002      Std  2012-10-12 02:00:00.0000000                    0.000\n",
       "4   MAC000002      Std  2012-10-12 02:30:00.0000000                    0.000\n",
       "..        ...      ...                          ...                      ...\n",
       "95  MAC000002      Std  2012-10-14 00:30:00.0000000                    0.166\n",
       "96  MAC000002      Std  2012-10-14 01:00:00.0000000                    0.226\n",
       "97  MAC000002      Std  2012-10-14 01:30:00.0000000                    0.088\n",
       "98  MAC000002      Std  2012-10-14 02:00:00.0000000                    0.126\n",
       "99  MAC000002      Std  2012-10-14 02:30:00.0000000                    0.082\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(path_to_data + '\\CC_LCL-FullData.csv', nrows=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to extract only data with 'std' value and remove all the 'ToU' users. Also there are some missing values in the data which are marked as 'Null', we will replace them with nan. This process will take a few minutes as we are concatenating all the data into one dataframe, it could be sped up by processing data in chunks, but we keep the data in one file for convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['id', 'datetime', 'consumption'])\n",
    "chunksize = 10**7\n",
    "dtypes = {'KWH/hh (per half hour) ': 'float64', 'LCLid': 'str', 'DateTime': 'datetime64'}\n",
    "with pd.read_csv(path_to_data + '\\CC_LCL-FullData.csv', \n",
    "                 chunksize=chunksize, dtype={'KWH/hh (per half hour) ': 'float64'},\n",
    "                 parse_dates=['DateTime'], na_values=['Null']) as reader:\n",
    "    for chunk in reader:\n",
    "        chunk = chunk[chunk['stdorToU']=='Std'].drop(columns=['stdorToU'])\n",
    "        chunk = chunk[['KWH/hh (per half hour) ', 'LCLid', 'DateTime']]\n",
    "        chunk = chunk.rename(columns={'KWH/hh (per half hour) ': 'consumption', 'LCLid': 'id', 'DateTime': 'datetime'})\n",
    "        chunk = chunk.groupby(['id', pd.Grouper(key='datetime', freq='H')]).sum().reset_index()\n",
    "        df = pd.concat([df, chunk], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190967\n",
      "4443\n"
     ]
    }
   ],
   "source": [
    "def count_missing_datetimes(group):\n",
    "    min_datetime = group['datetime'].min()\n",
    "    max_datetime = group['datetime'].max()\n",
    "    all_datetimes = pd.date_range(min_datetime, max_datetime, freq='1H')\n",
    "    missing = all_datetimes[~all_datetimes.isin(group['datetime'])]\n",
    "    return len(missing)\n",
    "    \n",
    "missing_counts = df.groupby('id').apply(count_missing_datetimes)\n",
    "print(missing_counts.sum())\n",
    "print(len(missing_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4443 customers in this dataset and close to 200k missing datetimes which we will add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_missing_datetimes(group):\n",
    "    min_datetime = group['datetime'].min()\n",
    "    max_datetime = group['datetime'].max()\n",
    "    all_datetimes = pd.date_range(min_datetime, max_datetime, freq='1H')\n",
    "    return pd.DataFrame({'id': group['id'].iloc[0], 'datetime': all_datetimes})\n",
    "\n",
    "df_all_datetimes = df.groupby('id').apply(generate_missing_datetimes).reset_index(drop=True)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df_all_datetimes = df_all_datetimes.merge(df, how='left', on=['id', 'datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_datetimes.to_csv('preprocessed_lcl.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c95252ec0fd7c5cb6f76d566d509abd2f82a93011f8cc5397bf70718e35dfb83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
