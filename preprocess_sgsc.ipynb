{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proprocessing Smart Grid Smart City (SGSC) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data used comes from Smart Grid Smart City (SGSC) project (2010-2014) and can be downloaded from the Austalian government website: https://data.gov.au/dataset/ds-dga-4e21dea3-9b87-4610-94c7-15a8a77907ef/details. We need to download the zipped readings data (*CD_INTERVAL_READING_ALL_NO_QUOTES.csv*), csv with description of each customer (*sgsc-customers.csv*) and data dictionary xlsx explaining fields in the csv (*sgsc-data-dictionary.xlsx*). From the list of all customers we choose the ones which are in the control group and don't have any automatically controlled load or solar panels energy as we don't want to take into account the effects of different tariffs or any additional factors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = r'path_to_your_folder_with_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198\n"
     ]
    }
   ],
   "source": [
    "customers = pd.read_csv(path_to_data + '\\sgsc-customers.csv')\n",
    "customers = customers[customers['CONTROL_GROUP_FLAG']=='Y']\n",
    "customers = customers[customers['CONTROLLED_LOAD_CNT']==0]\n",
    "customers = customers[customers['NET_SOLAR_CNT']==0]\n",
    "customers = customers[customers['GROSS_SOLAR_CNT']==0]\n",
    "customers = set(customers['CUSTOMER_KEY'])\n",
    "\n",
    "print(len(customers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with 1198 customers in the dataset for whom we extract the data. This process will take a few minutes as we are concatenating all the data into one dataframe, it could be sped up by processing data in chunks, but we keep the data in one file for convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 10**7\n",
    "df = pd.DataFrame(columns=['id', 'datetime', 'consumption'])\n",
    "\n",
    "with pd.read_csv(path_to_data + '\\CD_INTERVAL_READING_ALL_NO_QUOTES.csv', chunksize=chunksize, parse_dates=['READING_DATETIME']) as reader:\n",
    "    for chunk in reader:\n",
    "        chunk = chunk[['CUSTOMER_ID', 'READING_DATETIME', ' GENERAL_SUPPLY_KWH']]\n",
    "        chunk = chunk.rename(columns={' GENERAL_SUPPLY_KWH': 'consumption', 'READING_DATETIME': 'datetime', 'CUSTOMER_ID': 'id'})\n",
    "        df = pd.concat([df, chunk[chunk['id'].isin(customers)]], ignore_index=True)\n",
    "        \n",
    "df = df.groupby(['id', pd.Grouper(key='datetime', freq='H')]).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49494\n"
     ]
    }
   ],
   "source": [
    "def count_missing_datetimes(group):\n",
    "    min_datetime = group['datetime'].min()\n",
    "    max_datetime = group['datetime'].max()\n",
    "    all_datetimes = pd.date_range(min_datetime, max_datetime, freq='1H')\n",
    "    missing = all_datetimes[~all_datetimes.isin(group['datetime'])]\n",
    "    return len(missing)\n",
    "    \n",
    "missing_counts = df.groupby('id').apply(count_missing_datetimes)\n",
    "print(missing_counts.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is close to 50k missing datetimes so we add them and remove duplicate hours caused by the timezone change due to daylight saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['id', 'datetime'])\n",
    "\n",
    "def generate_missing_datetimes(group):\n",
    "    min_datetime = group['datetime'].min()\n",
    "    max_datetime = group['datetime'].max()\n",
    "    all_datetimes = pd.date_range(min_datetime, max_datetime, freq='1H')\n",
    "    return pd.DataFrame({'id': group['id'].iloc[0], 'datetime': all_datetimes})\n",
    "\n",
    "df_all_datetimes = df.groupby('id').apply(generate_missing_datetimes).reset_index(drop=True)  \n",
    "df_all_datetimes = df_all_datetimes.merge(df, how='left', on=['id', 'datetime'])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_datetimes.to_csv('preprocessed_sgsc.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c95252ec0fd7c5cb6f76d566d509abd2f82a93011f8cc5397bf70718e35dfb83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
